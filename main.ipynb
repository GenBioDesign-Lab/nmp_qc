{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236561af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import datasets\n",
    "from datasets import utils\n",
    "from models.MPNN import MPNN\n",
    "from LogMetric import AverageMeter, Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6093b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser check\n",
    "def restricted_float(x, inter):\n",
    "    x = float(x)\n",
    "    if x < inter[0] or x > inter[1]:\n",
    "        raise argparse.ArgumentTypeError(\"%r not in range [1e-5, 1e-4]\"%(x,))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b87b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration variables (default values)\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Dataset options\n",
    "        self.dataset = 'qm9'\n",
    "        self.datasetPath = './data/qm9/dsgdb9nsd/'\n",
    "        self.logPath = './log/qm9/mpnn/'\n",
    "        self.plotLr = False\n",
    "        self.plotPath = './plot/qm9/mpnn/'\n",
    "        self.resume = './checkpoint/qm9/mpnn/'\n",
    "        \n",
    "        # Optimization Options\n",
    "        self.batch_size = 100\n",
    "        self.no_cuda = False\n",
    "        self.epochs = 360\n",
    "        self.lr = 1e-3\n",
    "        self.lr_decay = 0.6\n",
    "        self.schedule = [0.1, 0.9]\n",
    "        self.momentum = 0.9\n",
    "        \n",
    "        # i/o\n",
    "        self.log_interval = 20\n",
    "        \n",
    "        # Accelerating\n",
    "        self.prefetch = 2\n",
    "\n",
    "args = Config()\n",
    "best_er1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd226ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is enabled\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# Load data\n",
    "root = args.datasetPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea6a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load molecules and make dataset\n",
    "files = [f for f in os.listdir(root) if os.path.isfile(os.path.join(root, f))]\n",
    "\n",
    "idx = np.random.permutation(len(files))\n",
    "idx = idx.tolist()\n",
    "\n",
    "valid_ids = [files[i] for i in idx[0:10000]]\n",
    "test_ids = [files[i] for i in idx[10000:20000]]\n",
    "train_ids = [files[i] for i in idx[20000:]]\n",
    "\n",
    "data_train = datasets.Qm9(root, train_ids, edge_transform=utils.qm9_edges, e_representation='raw_distance')\n",
    "data_valid = datasets.Qm9(root, valid_ids, edge_transform=utils.qm9_edges, e_representation='raw_distance')\n",
    "data_test = datasets.Qm9(root, test_ids, edge_transform=utils.qm9_edges, e_representation='raw_distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fc8a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and optimizer\n",
    "print('Define model')\n",
    "# Select one graph\n",
    "g_tuple, l = data_train[100]\n",
    "g, h_t, e = g_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c62a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\tStatistics')\n",
    "stat_dict = datasets.utils.get_graph_stats(data_valid, ['target_mean', 'target_std'])\n",
    "\n",
    "data_train.set_target_transform(lambda x: datasets.utils.normalize_data(x,stat_dict['target_mean'],\n",
    "                                                                        stat_dict['target_std']))\n",
    "data_valid.set_target_transform(lambda x: datasets.utils.normalize_data(x, stat_dict['target_mean'],\n",
    "                                                                        stat_dict['target_std']))\n",
    "data_test.set_target_transform(lambda x: datasets.utils.normalize_data(x, stat_dict['target_mean'],\n",
    "                                                                        stat_dict['target_std']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eb4dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "train_loader = torch.utils.data.DataLoader(data_train,\n",
    "                                            batch_size=args.batch_size, shuffle=True,\n",
    "                                            collate_fn=datasets.utils.collate_g,\n",
    "                                            num_workers=args.prefetch, pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(data_valid,\n",
    "                                            batch_size=args.batch_size, collate_fn=datasets.utils.collate_g,\n",
    "                                            num_workers=args.prefetch, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(data_test,\n",
    "                                          batch_size=args.batch_size, collate_fn=datasets.utils.collate_g,\n",
    "                                          num_workers=args.prefetch, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5b9396",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\tCreate model')\n",
    "in_n = [len(h_t[0]), len(list(e.values())[0])] # [13, 5]\n",
    "hidden_state_size = 73\n",
    "message_size = 73\n",
    "n_layers = 3\n",
    "l_target = len(l)\n",
    "type ='regression'\n",
    "model = MPNN(in_n, hidden_state_size, message_size, n_layers, l_target, type=type)\n",
    "del in_n, hidden_state_size, message_size, n_layers, l_target, type\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "evaluation = lambda output, target: torch.mean(torch.abs(output - target) / torch.abs(target))\n",
    "\n",
    "logger = Logger(args.logPath)\n",
    "\n",
    "lr_step = (args.lr-args.lr*args.lr_decay)/(args.epochs*args.schedule[1] - args.epochs*args.schedule[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578d6506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best checkpoint if available without training\n",
    "if args.resume:\n",
    "    checkpoint_dir = args.resume\n",
    "    best_model_file = os.path.join(checkpoint_dir, 'model_best.pth')\n",
    "    if not os.path.isdir(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if os.path.isfile(best_model_file):\n",
    "        print(\"=> loading best model '{}'\".format(best_model_file))\n",
    "        checkpoint = torch.load(best_model_file)\n",
    "        args.start_epoch = checkpoint['epoch']\n",
    "        best_acc1 = checkpoint['best_er1']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"=> loaded best model '{}' (epoch {})\".format(best_model_file, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no best model found at '{}'\".format(best_model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2acb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Check cuda')\n",
    "if args.cuda:\n",
    "    print('\\t* Cuda')\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcfac9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, evaluation, logger):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    error_ratio = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (g, h, e, target) in enumerate(train_loader):\n",
    "\n",
    "        # Prepare input data\n",
    "        if args.cuda:\n",
    "            g, h, e, target = g.cuda(), h.cuda(), e.cuda(), target.cuda()\n",
    "        g, h, e, target = Variable(g), Variable(h), Variable(e), Variable(target)\n",
    "\n",
    "        # Measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute output\n",
    "        output = model(g, h, e)\n",
    "        train_loss = criterion(output, target)\n",
    "\n",
    "        # Logs\n",
    "        losses.update(train_loss.item(), g.size(0))\n",
    "        error_ratio.update(evaluation(output, target).item(), g.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.log_interval == 0 and i > 0:\n",
    "\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Error Ratio {err.val:.4f} ({err.avg:.4f})'\n",
    "                  .format(epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                          data_time=data_time, loss=losses, err=error_ratio))\n",
    "                          \n",
    "    logger.log_value('train_epoch_loss', losses.avg)\n",
    "    logger.log_value('train_epoch_error_ratio', error_ratio.avg)\n",
    "\n",
    "    print('Epoch: [{0}] Avg Error Ratio {err.avg:.3f}; Average Loss {loss.avg:.3f}; Avg Time x Batch {b_time.avg:.3f}'\n",
    "          .format(epoch, err=error_ratio, loss=losses, b_time=batch_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393cf471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, evaluation, logger=None):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    error_ratio = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (g, h, e, target) in enumerate(val_loader):\n",
    "\n",
    "        # Prepare input data\n",
    "        if args.cuda:\n",
    "            g, h, e, target = g.cuda(), h.cuda(), e.cuda(), target.cuda()\n",
    "        g, h, e, target = Variable(g), Variable(h), Variable(e), Variable(target)\n",
    "\n",
    "        # Compute output\n",
    "        output = model(g, h, e)\n",
    "\n",
    "        # Logs\n",
    "        losses.update(criterion(output, target).item(), g.size(0))\n",
    "        error_ratio.update(evaluation(output, target).item(), g.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.log_interval == 0 and i > 0:\n",
    "            \n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Error Ratio {err.val:.4f} ({err.avg:.4f})'\n",
    "                  .format(i, len(val_loader), batch_time=batch_time,\n",
    "                          loss=losses, err=error_ratio))\n",
    "\n",
    "    print(' * Average Error Ratio {err.avg:.3f}; Average Loss {loss.avg:.3f}'\n",
    "          .format(err=error_ratio, loss=losses))\n",
    "\n",
    "    if logger is not None:\n",
    "        logger.log_value('test_epoch_loss', losses.avg)\n",
    "        logger.log_value('test_epoch_error_ratio', error_ratio.avg)\n",
    "\n",
    "    return error_ratio.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8690a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch for loop\n",
    "for epoch in tqdm(range(0, args.epochs), desc='Epoch'):\n",
    "\n",
    "    if epoch > args.epochs * args.schedule[0] and epoch < args.epochs * args.schedule[1]:\n",
    "        args.lr -= lr_step\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = args.lr\n",
    "\n",
    "    # train for one epoch\n",
    "    train(train_loader, model, criterion, optimizer, epoch, evaluation, logger)\n",
    "\n",
    "    # evaluate on test set\n",
    "    er1 = validate(valid_loader, model, criterion, evaluation, logger)\n",
    "\n",
    "    is_best = er1 > best_er1\n",
    "    best_er1 = min(er1, best_er1)\n",
    "    utils.save_checkpoint({'epoch': epoch + 1, 'state_dict': model.state_dict(), 'best_er1': best_er1,\n",
    "                            'optimizer': optimizer.state_dict(), }, is_best=is_best, directory=args.resume)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmp-qc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
